from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException

# ==============================
# üîß CONFIGURACI√ìN DE CONEXI√ìN
# ==============================
DB_HOST = "mariadb"
DB_PORT = "3306"
DB_NAME = "bigdata_db"
DB_USER = "bigdata_user"
DB_PASS = "bigdata_pass"
TABLE_NAME = "sensores"

# URL JDBC (MariaDB) con flags recomendados
JDBC_URL = (
    f"jdbc:mysql://{DB_HOST}:{DB_PORT}/{DB_NAME}"
    "?useUnicode=true"
    "&permitMysqlScheme=true"
    "&characterEncoding=utf8"
    "&serverTimezone=UTC"
    "&tinyInt1isBit=false"
    "&zeroDateTimeBehavior=convertToNull"
)

# ==============================
# üöÄ INICIAR SESI√ìN SPARK
# ==============================
spark = (
    SparkSession.builder
    .appName("TestMariaDB_Stable")
    # Asegurate de que este .jar est√© presente en /opt/spark/jars
    .config("spark.jars", "/opt/spark/jars/mariadb-java-client.jar")
    .getOrCreate()
)

# Fijar zona horaria de la sesi√≥n para columnas TIMESTAMP
spark.sql("SET spark.sql.session.timeZone=UTC")

print("üîå Intentando conectar a MariaDB...")
print(f"   ‚Üí URL: {JDBC_URL}")
print(f"   ‚Üí Tabla: {TABLE_NAME}")

# ==============================
# üì• LECTURA DESDE MARIADB
# ==============================
try:
    df = (
        spark.read.format("jdbc")
        .option("url", JDBC_URL)
        .option("dbtable", TABLE_NAME)
        .option("user", DB_USER)
        .option("password", DB_PASS)
        .option("driver", "org.mariadb.jdbc.Driver")
        .option("fetchsize", "1000")   # tuning para tablas grandes
        .load()
    )

    print("\n‚úÖ Conexi√≥n exitosa. Mostrando los primeros registros:\n")
    # Mostrar hasta 100 filas y sin truncar columnas
    df.show(100, truncate=False)

    print("\nüìä Esquema de la tabla:")
    df.printSchema()

    total = df.count()
    print(f"\nüìà Total de filas: {total}")

    # OPCIONAL: convertir a double para ciertos c√°lculos
    # df_num = df.selectExpr(
    #     "id",
    #     "dispositivo",
    #     "CAST(temperatura AS DOUBLE) AS temperatura",
    #     "CAST(humedad AS DOUBLE) AS humedad",
    #     "fecha"
    # )
    # df_num.printSchema()
    # df_num.show(100, truncate=False)

except AnalysisException as e:
    print(f"‚ö†Ô∏è Error de an√°lisis Spark: {e}")
except Exception as e:
    print(f"‚ùå Error general al conectar o leer datos: {e}")
finally:
    spark.stop()
    print("\nüßπ Sesi√≥n Spark finalizada correctamente.")
