#!/bin/bash
set -Eeuo pipefail

# ==========================================================
# üß† CONTROLADOR DE ENTORNO BIG DATA (Carlos Piriz)
#  - Modo por defecto: LOCAL (sin ngrok, n8n en http://localhost)
#  - Modo alternativo: P√öBLICO (ngrok + n8n HTTPS externo)
# ==========================================================

ENV_FILE=".env"
COMPOSE_FILE="docker-compose.yml"

# Nombre de proyecto para separar stacks en Docker Compose
BIGDATA_PROJECT_NAME="${BIGDATA_PROJECT_NAME:-bigdata}"

# Carpeta local de logs (solo para modo debug)
LOG_DIR="${LOG_DIR:-./volumenes/controller-logs}"

# üé® Colores
GREEN="\e[32m"
RED="\e[31m"
YELLOW="\e[33m"
CYAN="\e[36m"
BOLD="\e[1m"
RESET="\e[0m"

# ==========================================================
# ‚öôÔ∏è CARGAR VARIABLES .ENV
# ==========================================================
if [ -f "$ENV_FILE" ]; then
  echo -e "${YELLOW}‚öôÔ∏è  Cargando variables desde $ENV_FILE...${RESET}"
  # shellcheck disable=SC2046
  export $(grep -v '^#' "$ENV_FILE" | xargs)
else
  echo -e "${RED}‚ùå No se encontr√≥ $ENV_FILE. Abortando.${RESET}"
  exit 1
fi

# ==========================================================
# üß© VERIFICACI√ìN DE VOL√öMENES Y ARCHIVOS BASE
# ==========================================================
fix_volumes() {
  echo -e "${YELLOW}üß© Verificando estructura de vol√∫menes locales...${RESET}"

  mkdir -p volumenes/{superset,jupyterlab,shared,airflow-logs,airflow-plugins,redis-data,n8n-data}
  mkdir -p volumenes/shared/{dags_airflow,scripts_airflow,spark-events}

  echo -e "${YELLOW}üîß Ajustando permisos en carpetas de trabajo...${RESET}"
  chmod -R 777 \
    volumenes/superset \
    volumenes/jupyterlab \
    volumenes/shared \
    volumenes/airflow-logs \
    volumenes/airflow-plugins \
    volumenes/redis-data \
    volumenes/n8n-data 2>/dev/null || true
  echo -e "${GREEN}‚úÖ Permisos aplicados correctamente a carpetas de usuario.${RESET}"
  echo -e "${YELLOW}‚ö†Ô∏è  Carpetas protegidas (MariaDB y MinIO) no fueron modificadas para evitar errores.${RESET}"

  # Notebook de ejemplo
  if [ -f "./notebooks/sensores_demo.ipynb" ] && [ ! -f "./volumenes/jupyterlab/sensores_demo.ipynb" ]; then
    cp ./notebooks/sensores_demo.ipynb ./volumenes/jupyterlab/
    chmod 666 ./volumenes/jupyterlab/sensores_demo.ipynb
    echo -e "${GREEN}üìò Notebook de ejemplo copiado.${RESET}"
  fi

  # DAGs y scripts
  if [ -f "./plantillas/dag_test_mariadb.py" ]; then
    cp -f ./plantillas/dag_test_mariadb.py ./volumenes/shared/dags_airflow/
    chmod 666 ./volumenes/shared/dags_airflow/dag_test_mariadb.py
  fi
  if [ -f "./plantillas/test_mariadb.py" ]; then
    cp -f ./plantillas/test_mariadb.py ./volumenes/shared/scripts_airflow/
    chmod 666 ./volumenes/shared/scripts_airflow/test_mariadb.py
  fi

  echo -e "${GREEN}‚úÖ Vol√∫menes y archivos base verificados.${RESET}"
}


# ==========================================================
# üìç RUTA BASE DEL SCRIPT (independiente del pwd)
# ==========================================================
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# ==========================================================
# üß¨ GENERACI√ìN DE SQL DESDE TEMPLATE
# ==========================================================
SQL_TEMPLATE="${SCRIPT_DIR}/init-sql/00-init-all.sql.template"
SQL_OUTPUT="${SCRIPT_DIR}/init-sql/00-init-all.sql"



generate_init_sql() {
  echo -e "${YELLOW}üß¨ Generando SQL de inicializaci√≥n desde template...${RESET}"

  local required_vars=(
    MARIADB_USER
    MARIADB_PASSWORD
    MARIADB_DATABASE
    SUPERSET_DB_NAME
    AIRFLOW_DB_NAME
  )

  for var in "${required_vars[@]}"; do
    if [ -z "${!var:-}" ]; then
      echo -e "${RED}‚ùå Variable requerida no definida: $var${RESET}"
      exit 1
    fi
  done

  if [ ! -f "$SQL_TEMPLATE" ]; then
    echo -e "${RED}‚ùå No se encontr√≥ el template $SQL_TEMPLATE${RESET}"
    exit 1
  fi

  export GENERATED_AT
  GENERATED_AT="$(date '+%Y-%m-%d %H:%M:%S')"

  envsubst < "$SQL_TEMPLATE" > "$SQL_OUTPUT"

  echo -e "${GREEN}‚úÖ SQL generado correctamente: $SQL_OUTPUT${RESET}"
}


# ==========================================================
# ‚è≥ UTILIDADES DE ESPERA
# ==========================================================
wait_for_http() {
  local port="$1"
  local path="${2:-/}"
  local label="${3:-Servicio HTTP}"
  local max_attempts=15
  local sleep_time=5

  echo -e "${YELLOW}‚è≥ Esperando ${label} en http://localhost:${port}${path} ...${RESET}"
  for i in $(seq 1 "${max_attempts}"); do
    if curl -sf "http://localhost:${port}${path}" >/dev/null 2>&1; then
      echo -e "${GREEN}‚úÖ ${label} respondi√≥.${RESET}"
      return 0
    fi
    echo -e "${YELLOW}‚åõ Intento ${i}/${max_attempts} - esperando ${sleep_time}s...${RESET}"
    sleep "${sleep_time}"
  done
  echo -e "${RED}‚ùå ${label} no respondi√≥ a tiempo.${RESET}"
  return 1
}

wait_for_service() {
  local container="$1"
  local message="$2"
  local max_attempts=15
  local sleep_time=5

  echo -e "${YELLOW}‚è≥ Esperando ${message}...${RESET}"

  for i in $(seq 1 "${max_attempts}"); do
    # Caso especial: Spark Master por puerto de UI (mapeado al host)
    if [[ "${container}" == "spark-master" ]]; then
      if curl -sf "http://localhost:${SPARK_MASTER_WEBUI_PORT}" >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ ${message} est√° listo (UI en puerto ${SPARK_MASTER_WEBUI_PORT}).${RESET}"
        return 0
      fi
    # Caso general: contenedores con healthcheck
    elif docker ps --filter "name=${container}" --filter "health=healthy" | grep -q "${container}"; then
      echo -e "${GREEN}‚úÖ ${message} est√° listo.${RESET}"
      return 0
    fi
    echo -e "${YELLOW}‚åõ Intento ${i}/${max_attempts} - esperando ${sleep_time}s...${RESET}"
    sleep "${sleep_time}"
  done

  echo -e "${RED}‚ùå ${message} no respondi√≥ a tiempo.${RESET}"
  return 1
}

# ‚úÖ FLOWER
wait_for_flower() {
  local name="airflow-flower"
  local max_attempts=15
  local sleep_time=5

  if ! docker ps -a --format '{{.Names}}' | grep -q "^${name}$"; then
    return 0
  fi

  echo -e "${YELLOW}‚è≥ Esperando Flower UI...${RESET}"
  for i in $(seq 1 "${max_attempts}"); do
    if docker inspect "${name}" --format '{{.State.Health.Status}}' >/dev/null 2>&1; then
      if docker ps --filter "name=${name}" --filter "health=healthy" | grep -q "${name}"; then
        echo -e "${GREEN}‚úÖ Flower UI listo (health=healthy).${RESET}"
        return 0
      fi
    else
      if curl -sf "http://localhost:5555/" >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ Flower UI respondi√≥ por HTTP (5555).${RESET}"
        return 0
      fi
    fi
    echo -e "${YELLOW}‚åõ Intento ${i}/${max_attempts} - esperando ${sleep_time}s...${RESET}"
    sleep "${sleep_time}"
  done
  echo -e "${RED}‚ùå Flower UI no respondi√≥ a tiempo.${RESET}"
  return 1
}

# ‚úÖ N8N (chequeo health)
wait_for_n8n() {
  local max_attempts=30
  local sleep_time=3

  # Detecta puerto publicado en el host para 5678/tcp
  local host_port
  host_port="$(docker inspect n8n -f '{{range $p, $cfg := .NetworkSettings.Ports}}{{if eq $p "5678/tcp"}}{{(index $cfg 0).HostPort}}{{end}}{{end}}' 2>/dev/null || true)"
  [ -z "${host_port}" ] && host_port="5678"

  local url_local="http://localhost:${host_port}"

  echo -e "${YELLOW}‚è≥ Esperando interfaz de n8n en ${url_local} ...${RESET}"
  for i in $(seq 1 "${max_attempts}"); do
    node -e "require('http').get('${url_local}/rest/health',r=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))" \
      || node -e "require('http').get('${url_local}',r=>process.exit([200,301,302,401].includes(r.statusCode)?0:1)).on('error',()=>process.exit(1))"
    if [ $? -eq 0 ]; then
      echo -e "${GREEN}‚úÖ n8n respondi√≥ correctamente en ${url_local}.${RESET}"
      return 0
    fi
    echo -e "${YELLOW}‚åõ Intento ${i}/${max_attempts} - esperando ${sleep_time}s...${RESET}"
    sleep "${sleep_time}"
  done
  echo -e "${RED}‚ùå n8n no respondi√≥ a tiempo en ${url_local}.${RESET}"
  return 1
}

# ==========================================================
# üîå NGROK PARA N8N (modo p√∫blico)
# ==========================================================
NGROK_PIDFILE=".ngrok-n8n.pid"

stop_ngrok_for_n8n() {
  if [ -f "${NGROK_PIDFILE}" ]; then
    local PID
    PID="$(cat "${NGROK_PIDFILE}" || true)"
    if [ -n "${PID:-}" ] && ps -p "${PID}" >/dev/null 2>&1; then
      echo -e "${YELLOW}üîå Cerrando t√∫nel ngrok (PID ${PID})...${RESET}"
      kill "${PID}" 2>/dev/null || true
      sleep 1
    fi
    rm -f "${NGROK_PIDFILE}"
  fi
}

start_ngrok_for_n8n() {
  local ngrok_bin="${N8N_NGROK_BIN:-ngrok}"
  local port="${N8N_NGROK_PORT:-${N8N_PORT:-5678}}"
  local ngrok_log=".ngrok-n8n.log"

  stop_ngrok_for_n8n

  if ! command -v "${ngrok_bin}" >/dev/null 2>&1; then
    echo -e "${RED}‚ùå No se encontr√≥ '${ngrok_bin}'. Instalalo o ajust√° N8N_NGROK_BIN en .env.${RESET}"
    return 0
  fi

  local domain_arg=()
  if [ -n "${N8N_NGROK_DOMAIN:-}" ]; then
    domain_arg=(--domain="${N8N_NGROK_DOMAIN}")
    echo -e "${YELLOW}üîó Iniciando ngrok con dominio fijo: ${N8N_NGROK_DOMAIN}${RESET}"
  else
    echo -e "${YELLOW}üîó Iniciando ngrok con subdominio din√°mico (free).${RESET}"
  fi

  nohup "${ngrok_bin}" http "${domain_arg[@]}" "${port}" --log=stdout >"${ngrok_log}" 2>&1 &
  echo $! > "${NGROK_PIDFILE}"
  echo -e "${GREEN}‚úÖ ngrok iniciado (PID $(cat "${NGROK_PIDFILE}")). Log: ${ngrok_log}${RESET}"

  local max_attempts=12
  local sleep_time=1
  local ok_api=0
  for i in $(seq 1 "${max_attempts}"); do
    if curl -sf http://127.0.0.1:4040/api/tunnels >/dev/null 2>&1; then
      ok_api=1
      break
    fi
    sleep "${sleep_time}"
  done

  if [ "${ok_api}" -eq 1 ]; then
    local public_url
    local domain
    public_url="$(curl -sf http://127.0.0.1:4040/api/tunnels | grep -o '"public_url":"https:[^"]*"' | head -n1 | sed 's/"public_url":"\(.*\)"/\1/')"
    if [ -n "${public_url}" ]; then
      domain="$(echo "${public_url}" | sed -E 's#https?://([^/]+).*#\1#')"
      export N8N_WEBHOOK_URL="${public_url}"
      export N8N_HOST="${domain}"
      export N8N_PROTOCOL="https"
      export N8N_PORT="443"
      echo -e "${GREEN}üåê ngrok URL:${RESET} ${CYAN}${public_url}${RESET}"
      echo -e "${YELLOW}‚Ü™ Exportadas para esta sesi√≥n:${RESET}"
      echo -e "   N8N_WEBHOOK_URL=${N8N_WEBHOOK_URL}"
      echo -e "   N8N_HOST=${N8N_HOST}"
      echo -e "   N8N_PROTOCOL=${N8N_PROTOCOL}"
      echo -e "   N8N_PORT=${N8N_PORT}"
    else
      echo -e "${YELLOW}‚ö†Ô∏è ngrok corriendo, pero a√∫n no publica URL. Segu√≠s. Revis√° ${ngrok_log} o http://127.0.0.1:4040${RESET}"
    fi
  else
    echo -e "${YELLOW}‚ö†Ô∏è No respondi√≥ la API de ngrok (4040) a tiempo. Segu√≠s. Log: ${ngrok_log}${RESET}"
  fi
}

# ==========================================================
# üß™ INFO √öTIL (OAuth y URL p√∫blica)
# ==========================================================
print_n8n_oauth_info() {
  local proto="${N8N_PROTOCOL:-http}"
  local host="${N8N_HOST:-localhost}"
  local port="${N8N_PORT:-5678}"

  local base="${N8N_WEBHOOK_URL:-${proto}://${host}:${port}}"
  base="${base%/}"
  local redirect="${base}/rest/oauth2-credential/callback"
  local origin
  origin="$(echo "${base}" | sed -E 's#(https?://[^/]+).*#\1#')"

  echo -e ""
  echo -e "${CYAN}${BOLD}üîë OAuth (n8n) ‚Äì Peg√° en Google Cloud:${RESET}"
  echo -e "   ‚Ä¢ Authorized redirect URIs:        ${GREEN}${redirect}${RESET}"
  echo -e "   ‚Ä¢ Authorized JavaScript origins:   ${GREEN}${origin}${RESET}"
  echo -e ""
}

print_ngrok_public_url() {
  local base="${N8N_WEBHOOK_URL:-}"
  if [ -z "${base}" ] && [ -n "${N8N_HOST:-}" ] && [ -n "${N8N_PROTOCOL:-}" ]; then
    base="${N8N_PROTOCOL}://${N8N_HOST}"
  fi
  [ -z "${base}" ] && return 0
  base="${base%/}"
  echo -e "${CYAN}${BOLD}üåç n8n (externo via ngrok):${RESET} ${GREEN}${base}${RESET}  ${YELLOW}(BasicAuth ${N8N_BASIC_AUTH_USER:-admin}/${N8N_BASIC_AUTH_PASSWORD:-admin})${RESET}"
}

# ==========================================================
# ‚úÖ CHEQUEOS DE SERVICIOS (DB/Airflow)
# ==========================================================
check_mariadb() {
  echo -e "${YELLOW}üîç Verificando conexi√≥n a MariaDB...${RESET}"
  if docker exec mariadb mariadb -u"${MARIADB_USER}" -p"${MARIADB_PASSWORD}" -e "SELECT 1;" >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ MariaDB responde correctamente.${RESET}"
  else
    echo -e "${RED}‚ùå Error al conectar con MariaDB.${RESET}"
  fi
}

check_airflow_db_init() {
  echo -e "${YELLOW}üß† Verificando inicializaci√≥n de base Airflow...${RESET}"
  if ! docker exec airflow-webserver airflow db check-migrations >/dev/null 2>&1; then
    echo -e "${YELLOW}üß© Ejecutando airflow db init (primera vez)...${RESET}"
    if docker exec airflow-webserver airflow db init >/dev/null 2>&1; then
      echo -e "${GREEN}‚úÖ Base de datos Airflow inicializada.${RESET}"
    else
      echo -e "${RED}‚ùå Error inicializando la base de Airflow.${RESET}"
    fi
  else
    echo -e "${GREEN}‚úÖ Base de datos Airflow ya inicializada.${RESET}"
  fi
}

ensure_airflow_connection() {
  echo -e "${YELLOW}üîó Creando conexi√≥n 'spark_default' en Airflow (si no existe)...${RESET}"
  if docker exec airflow-webserver airflow connections get spark_default >/dev/null 2>&1; then
    echo -e "${GREEN}‚úÖ Conexi√≥n 'spark_default' ya existe.${RESET}"
  else
    if docker exec airflow-webserver bash -lc "airflow connections add spark_default --conn-uri 'spark://spark-master:7077'" >/dev/null 2>&1; then
      echo -e "${GREEN}‚úÖ Conexi√≥n 'spark_default' creada.${RESET}"
    else
      echo -e "${RED}‚ùå No se pudo crear 'spark_default'. Revis√° logs del webserver.${RESET}"
    fi
  fi
}

# ==========================================================
# üöÄ INICIAR ENTORNO BIG DATA (MODO LOCAL, SIN NGROK)  [POR DEFECTO]
# ==========================================================
start_services_local() {
  echo -e "${CYAN}${BOLD}üöÄ Iniciando entorno Big Data (modo LOCAL, sin ngrok)...${RESET}"
  generate_init_sql
  fix_volumes

  # NO arrancamos ngrok en modo local
  stop_ngrok_for_n8n

  local local_webhook="http://localhost:${N8N_PORT:-5678}"
  export N8N_PROTOCOL="http"
  export N8N_HOST="localhost"
  export N8N_WEBHOOK_URL="${local_webhook}"
  export WEBHOOK_URL="${local_webhook}"
  export WEBHOOK_TUNNEL_URL="${local_webhook}"
  export N8N_EDITOR_BASE_URL="${local_webhook}"

  if [[ "${DEBUG_BUILD:-0}" == "1" ]]; then
    mkdir -p "$LOG_DIR"
    LOG_FILE="$LOG_DIR/debug_build_$(date +%Y%m%d_%H%M%S).log"
    echo -e "${CYAN}üêõ DEBUG: Construyendo im√°genes (no-cache) con salida detallada...${RESET}"
    echo -e "${CYAN}üìù Log: $LOG_FILE${RESET}"
    COMPOSE_PROJECT_NAME="${BIGDATA_PROJECT_NAME}" docker compose --env-file "${ENV_FILE}" -f "${COMPOSE_FILE}" build --no-cache --progress=plain 2>&1 | tee "$LOG_FILE"

    echo -e "${CYAN}üêõ DEBUG: Levantando servicios (up -d)...${RESET}"
    COMPOSE_PROJECT_NAME="${BIGDATA_PROJECT_NAME}" docker compose --env-file "${ENV_FILE}" -f "${COMPOSE_FILE}" up -d 2>&1 | tee -a "$LOG_FILE"

  else
    COMPOSE_PROJECT_NAME="${BIGDATA_PROJECT_NAME}" docker compose --env-file "${ENV_FILE}" -f "${COMPOSE_FILE}" up -d --build
  fi

  wait_for_service mariadb "MariaDB"
  check_mariadb

  wait_for_service spark-master "Spark Master"
  wait_for_service superset "Superset"
  wait_for_service jupyterlab "JupyterLab"

  local AIRFLOW_HTTP_PORT
  AIRFLOW_HTTP_PORT="${AIRFLOW_PORT:-8090}"
  wait_for_http "${AIRFLOW_HTTP_PORT}" "/health" "Airflow Webserver"
  wait_for_flower

  wait_for_n8n

  check_airflow_db_init
  ensure_airflow_connection

  print_n8n_oauth_info

  echo -e ""
  echo -e "${GREEN}‚úÖ Todos los servicios Big Data (modo LOCAL) est√°n listos y verificados.${RESET}"
  echo -e ""
  echo -e "${CYAN}${BOLD}üåê SERVICIOS BIG DATA (MODO LOCAL):${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}MariaDB (Adminer):${RESET} ${GREEN}http://localhost:8089${RESET}   ${YELLOW}(user:${MARIADB_USER} / pass:${MARIADB_PASSWORD})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}MinIO Console:${RESET}  ${GREEN}http://localhost:${MINIO_CONSOLE_PORT}${RESET}   ${YELLOW}(user:${MINIO_ROOT_USER} / pass:${MINIO_ROOT_PASSWORD})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Spark Master:${RESET}   ${GREEN}http://localhost:${SPARK_MASTER_WEBUI_PORT}${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Spark History:${RESET}  ${GREEN}http://localhost:${SPARK_HISTORY_PORT}${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Airflow Web:${RESET}    ${GREEN}http://localhost:${AIRFLOW_HTTP_PORT}${RESET}   ${YELLOW}(user:${AIRFLOW_ADMIN_USER} / pass:${AIRFLOW_ADMIN_PASS})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Flower UI:${RESET}      ${GREEN}http://localhost:5555${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Superset:${RESET}       ${GREEN}http://localhost:${SUPERSET_PORT}${RESET}   ${YELLOW}(user:${SUPERSET_ADMIN_USER} / pass:${SUPERSET_ADMIN_PASSWORD})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}JupyterLab:${RESET}     ${GREEN}http://localhost:${JUPYTER_PORT}${RESET}   ${YELLOW}(token:${JUPYTER_TOKEN})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}n8n (local, sin ngrok):${RESET} ${GREEN}${local_webhook}${RESET}   ${YELLOW}(user:${N8N_BASIC_AUTH_USER} / pass:${N8N_BASIC_AUTH_PASSWORD})${RESET}"
}

# ==========================================================
# üöÄ INICIAR ENTORNO BIG DATA (MODO P√öBLICO CON NGROK)
# ==========================================================
start_services_public() {
  echo -e "${CYAN}${BOLD}üöÄ Iniciando entorno Big Data (modo P√öBLICO, ngrok + HTTPS)...${RESET}"
  generate_init_sql
  fix_volumes

  start_ngrok_for_n8n

  COMPOSE_PROJECT_NAME="${BIGDATA_PROJECT_NAME}" docker compose --env-file "${ENV_FILE}" -f "${COMPOSE_FILE}" up -d --build

  wait_for_service mariadb "MariaDB"
  check_mariadb

  wait_for_service spark-master "Spark Master"
  wait_for_service superset "Superset"
  wait_for_service jupyterlab "JupyterLab"

  local AIRFLOW_HTTP_PORT
  AIRFLOW_HTTP_PORT="${AIRFLOW_PORT:-8090}"
  wait_for_http "${AIRFLOW_HTTP_PORT}" "/health" "Airflow Webserver"
  wait_for_flower

  wait_for_n8n

  check_airflow_db_init
  ensure_airflow_connection

  print_n8n_oauth_info
  print_ngrok_public_url

  echo -e ""
  echo -e "${GREEN}‚úÖ Todos los servicios Big Data (modo P√öBLICO) est√°n listos y verificados.${RESET}"
  echo -e ""
  echo -e "${CYAN}${BOLD}üåê SERVICIOS BIG DATA (MODO P√öBLICO):${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}MariaDB (Adminer):${RESET} ${GREEN}http://localhost:8089${RESET}   ${YELLOW}(user:${MARIADB_USER} / pass:${MARIADB_PASSWORD})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}MinIO Console:${RESET}  ${GREEN}http://localhost:${MINIO_CONSOLE_PORT}${RESET}   ${YELLOW}(user:${MINIO_ROOT_USER} / pass:${MINIO_ROOT_PASSWORD})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Spark Master:${RESET}   ${GREEN}http://localhost:${SPARK_MASTER_WEBUI_PORT}${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Spark History:${RESET}  ${GREEN}http://localhost:${SPARK_HISTORY_PORT}${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Airflow Web:${RESET}    ${GREEN}http://localhost:${AIRFLOW_HTTP_PORT}${RESET}   ${YELLOW}(user:${AIRFLOW_ADMIN_USER} / pass:${AIRFLOW_ADMIN_PASS})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Flower UI:${RESET}      ${GREEN}http://localhost:5555${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}Superset:${RESET}       ${GREEN}http://localhost:${SUPERSET_PORT}${RESET}   ${YELLOW}(user:${SUPERSET_ADMIN_USER} / pass:${SUPERSET_ADMIN_PASSWORD})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}JupyterLab:${RESET}     ${GREEN}http://localhost:${JUPYTER_PORT}${RESET}   ${YELLOW}(token:${JUPYTER_TOKEN})${RESET}"
  echo -e "‚û°Ô∏è  ${BOLD}n8n (local):${RESET}     ${GREEN}http://localhost:${N8N_PORT}${RESET}   ${YELLOW}(user:${N8N_BASIC_AUTH_USER} / pass:${N8N_BASIC_AUTH_PASSWORD})${RESET}"
}

# ==========================================================
# üõë DETENER ENTORNO BIG DATA
# ==========================================================
stop_services() {
  echo -e "${YELLOW}üõë Deteniendo contenedores Big Data...${RESET}"
  COMPOSE_PROJECT_NAME="${BIGDATA_PROJECT_NAME}" docker compose --env-file "${ENV_FILE}" -f "${COMPOSE_FILE}" down
  stop_ngrok_for_n8n
  echo -e "${GREEN}‚úÖ Entorno Big Data detenido correctamente.${RESET}"
}

# ==========================================================
# üìä ESTADO ACTUAL BIG DATA
# ==========================================================
status_services() {
  echo -e "${YELLOW}üìä Estado actual de contenedores Big Data:${RESET}"
  COMPOSE_PROJECT_NAME="${BIGDATA_PROJECT_NAME}" docker compose --env-file "${ENV_FILE}" -f "${COMPOSE_FILE}" ps
}

# ==========================================================
# üßπ LIMPIEZA PARCIAL
# ==========================================================
clean() {
  echo -e "${YELLOW}üßπ Limpiando logs de Airflow y eventos de Spark...${RESET}"
  rm -rf volumenes/airflow-logs/* volumenes/shared/spark-events/* 2>/dev/null || true
  echo -e "${GREEN}‚úÖ Limpieza parcial completada.${RESET}"
}

# ==========================================================
# üí£ LIMPIEZA TOTAL
# ==========================================================
full_clean() {
  echo -e "${RED}${BOLD}‚ö†Ô∏è  ESTA ACCI√ìN ELIMINA TODO:${RESET}"
  echo -e "   - Contenedores, vol√∫menes e im√°genes locales"
  echo -e "   - Carpeta ./volumenes"
  read -r -p "¬øContinuar? (escribe 'SI' para confirmar): " confirm
  if [ "${confirm}" != "SI" ]; then
    echo -e "${YELLOW}‚ùå Cancelado.${RESET}"
    exit 0
  fi
  COMPOSE_PROJECT_NAME="${BIGDATA_PROJECT_NAME}" docker compose --env-file "${ENV_FILE}" -f "${COMPOSE_FILE}" down --remove-orphans || true
  docker volume prune --all --force || true
  docker rmi -f $(docker images -q) 2>/dev/null || true
  sudo rm -rf ./volumenes || true
  stop_ngrok_for_n8n
  echo -e "${GREEN}‚úÖ Limpieza total completada.${RESET}"
}

# ==========================================================
# MENU PRINCIPAL
# ==========================================================
case "${1:-}" in
  ""|up)
    if [[ "${2:-}" == "--debug-build" || "${2:-}" == "debug" ]]; then
      DEBUG_BUILD=1 start_services_local
    else
      start_services_local
    fi
    ;;
  up-public)
    start_services_public
    ;;
  down)
    stop_services
    ;;
  status)
    status_services
    ;;
  clean)
    clean
    ;;
  full-clean)
    full_clean
    ;;
  *)
    echo -e "${YELLOW}Uso:${RESET} ./controller.sh {up [--debug-build]|up-public|down|status|clean|full-clean}"
    echo -e "   (sin par√°metro)  -> Big Data + n8n LOCAL (sin ngrok, http://localhost)"
    echo -e "   up               -> igual que sin par√°metro (modo local)"
    echo -e "   up --debug-build -> (LOCAL) rebuild con logs completos de build (RUN echo, etc.) + log en $LOG_DIR"
    echo -e "   up-public        -> Big Data + n8n P√öBLICO (ngrok + HTTPS)"
    ;;
esac